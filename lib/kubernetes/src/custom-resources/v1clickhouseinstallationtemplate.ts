/**
 * Copyright 2021 Opstrace, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/* eslint-disable */
import localVarRequest from "request";
import { Request } from "request";
import {
  createReducer,
  createAsyncAction,
  ActionType,
  createAction
} from "typesafe-actions";
import {
  ApiKeyAuth,
  Authentication,
  ObjectSerializer,
  K8sResource,
  isSameObject,
  ResourceCache,
  VoidAuth } from "../common";
import { IncomingMessage } from "http";
import {
  V1Status,
  V1ListMeta,
  KubeConfig,
  Watch,
  Interceptor
} from "@kubernetes/client-node";
import { log } from "@opstrace/utils";

// ===============================================
// This file is autogenerated - Please do not edit
// ===============================================

/* tslint:disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * define a template which describe Kubernetes resources (StatefulSet, PVC, Service, ConfigMap) which describe behavior one or more ClickHouse clusters, would be used in `chi.spec.useTemplates`
 */
export interface V1Clickhouseinstallationtemplate {
  /**
   * APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
   */
  apiVersion?: string;
  /**
   * Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
   */
  kind?: string;
  metadata?: {
    [k: string]: any;
  };
  /**
   * Current ClickHouseInstallation manifest status, contains many fields like a normalized configuration, clickhouse-operator version, current action and all applied action list, current taskID and all applied taskIDs and other
   */
  status?: {
    [k: string]: any;
  };
  /**
   * Specification of the desired behavior of one or more ClickHouse clusters
   * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md"
   *
   */
  spec: {
    /**
     * Allow define custom taskID for named update and watch status of this update execution in .status.taskIDs field, by default every update of chi manifest will generate random taskID
     */
    taskID?: string;
    /**
     * Allow stop all ClickHouse clusters described in current chi.
     * Stop mechanism works as follows:
     *  - When `stop` is `1` then setup `Replicas: 0` in each related to current `chi` StatefulSet resource, all `Pods` and `Service` resources will desctroy, but PVCs still live
     *  - When `stop` is `0` then `Pods` will created again and will attach retained PVCs and `Service` also will created again
     *
     */
    stop?:
      | ""
      | "0"
      | "1"
      | "False"
      | "false"
      | "True"
      | "true"
      | "No"
      | "no"
      | "Yes"
      | "yes"
      | "Off"
      | "off"
      | "On"
      | "on"
      | "Disable"
      | "disable"
      | "Enable"
      | "enable"
      | "Disabled"
      | "disabled"
      | "Enabled"
      | "enabled";
    /**
     * restart policy for StatefulSets. When value `RollingUpdate` it allow graceful restart one by one instead of restart all StatefulSet simultaneously
     */
    restart?: "" | "RollingUpdate";
    /**
     * allows troubleshoot Pods during CrashLoopBack state, when you apply wrong configuration, `clickhouse-server` wouldn't startup
     */
    troubleshoot?:
      | ""
      | "0"
      | "1"
      | "False"
      | "false"
      | "True"
      | "true"
      | "No"
      | "no"
      | "Yes"
      | "yes"
      | "Off"
      | "off"
      | "On"
      | "on"
      | "Disable"
      | "disable"
      | "Enable"
      | "enable"
      | "Disabled"
      | "disabled"
      | "Enabled"
      | "enabled";
    /**
     * custom domain suffix which will add to end of `Service` or `Pod` name, use it when you use custom cluster domain in your Kubernetes cluster
     */
    namespaceDomainPattern?: string;
    /**
     * optional, define policy for auto applying ClickHouseInstallationTemplate inside ClickHouseInstallation
     */
    templating?: {
      /**
       * when defined as `auto` inside ClickhouseInstallationTemplate, it will auto add into all ClickHouseInstallation, manual value is default
       */
      policy?: "auto" | "manual";
      [k: string]: any;
    };
    /**
     * optional, allows tuning reconciling cycle for ClickhouseInstallation from clickhouse-operator side
     */
    reconciling?: {
      policy?: string;
      /**
       * timeout in seconds when `clickhouse-operator` will wait when applied `ConfigMap` during reconcile `ClickhouseInstallation` pods will updated from cache
       * see details: https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically
       *
       */
      configMapPropagationTimeout?: number;
      /**
       * optional, define behavior for cleanup Kubernetes resources during reconcile cycle
       */
      cleanup?: {
        /**
         * what clickhouse-operator shall do when found Kubernetes resources which should be managed with clickhouse-operator, but not have `ownerReference` to any currently managed `ClickHouseInstallation` resource, default behavior is `Delete`
         */
        unknownObjects?: {
          /**
           * behavior policy for unknown StatefulSet, Delete by default
           */
          statefulSet?: "Retain" | "Delete";
          /**
           * behavior policy for unknown PVC, Delete by default
           */
          pvc?: "Retain" | "Delete";
          /**
           * behavior policy for unknown ConfigMap, Delete by default
           */
          configMap?: "Retain" | "Delete";
          /**
           * behavior policy for unknown Service, Delete by default
           */
          service?: "Retain" | "Delete";
          [k: string]: any;
        };
        /**
         * what clickhouse-operator shall do when reconciling Kubernetes resources are failed, default behavior is `Retain`
         */
        reconcileFailedObjects?: {
          /**
           * behavior policy for failed StatefulSet reconciling, Retain by default
           */
          statefulSet?: "Retain" | "Delete";
          /**
           * behavior policy for failed PVC reconciling, Retain by default
           */
          pvc?: "Retain" | "Delete";
          /**
           * behavior policy for failed ConfigMap reconciling, Retain by default
           */
          configMap?: "Retain" | "Delete";
          /**
           * behavior policy for failed Service reconciling, Retain by default
           */
          service?: "Retain" | "Delete";
          [k: string]: any;
        };
        [k: string]: any;
      };
      [k: string]: any;
    };
    /**
     * define default behavior for whole ClickHouseInstallation, some behavior can be re-define on cluster, shard and replica level
     * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specdefaults
     *
     */
    defaults?: {
      /**
       * define should replicas be specified by FQDN in `<host></host>`, then "no" then will use short hostname and clickhouse-server will use kubernetes default suffixes for properly DNS lookup
       * "yes" by default
       *
       */
      replicasUseFQDN?:
        | ""
        | "0"
        | "1"
        | "False"
        | "false"
        | "True"
        | "true"
        | "No"
        | "no"
        | "Yes"
        | "yes"
        | "Off"
        | "off"
        | "On"
        | "on"
        | "Disable"
        | "disable"
        | "Enable"
        | "enable"
        | "Disabled"
        | "disabled"
        | "Enabled"
        | "enabled";
      /**
       * allows change `<yandex><distributed_ddl></distributed_ddl></yandex>` settings
       * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-distributed_ddl
       *
       */
      distributedDDL?: {
        /**
         * Settings from this profile will be used to execute DDL queries
         */
        profile?: string;
        [k: string]: any;
      };
      /**
       * optional, configuration of the templates names which will use for generate Kubernetes resources according to one or more ClickHouse clusters described in current ClickHouseInstallation (chi) resource
       */
      templates?: {
        /**
         * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
         */
        hostTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
         */
        podTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
         */
        dataVolumeClaimTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
         */
        logVolumeClaimTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
         */
        serviceTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
         */
        clusterServiceTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
         */
        shardServiceTemplate?: string;
        /**
         * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
         */
        replicaServiceTemplate?: string;
        /**
         * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
         */
        volumeClaimTemplate?: string;
        [k: string]: any;
      };
      [k: string]: any;
    };
    /**
     * allows configure multiple aspects and behavior for `clickhouse-server` instance and also allows describe multiple `clickhouse-server` clusters inside one `chi` resource
     */
    configuration?: {
      /**
       * allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
       * `clickhouse-operator` itself doesn't manage Zookeeper, please install Zookeeper separatelly look examples on https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/
       * currently, zookeeper (or clickhouse-keeper replacement) used for *ReplicatedMergeTree table engines and for `distributed_ddl`
       * More details: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings_zookeeper
       *
       */
      zookeeper?: {
        /**
         * describe every available zookeeper cluster node for interaction
         */
        nodes?: {
          /**
           * dns name or ip address for Zookeeper node
           */
          host?: string;
          /**
           * TCP port which used to connect to Zookeeper node
           */
          port?: number;
          [k: string]: any;
        }[];
        /**
         * session timeout during connect to Zookeeper
         */
        session_timeout_ms?: number;
        /**
         * one operation timeout during Zookeeper transactions
         */
        operation_timeout_ms?: number;
        /**
         * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
         */
        root?: string;
        /**
         * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
         */
        identity?: string;
        [k: string]: any;
      };
      /**
       * allows configure <yandex><users>..</users></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
       * you can configure password hashed, authorization restrictions, database level security row filters etc.
       * More details: https://clickhouse.tech/docs/en/operations/settings/settings-users/
       * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationusers
       *
       */
      users?: {
        [k: string]: any;
      };
      /**
       * allows configure <yandex><profiles>..</profiles></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
       * you can configure any aspect of settings profile
       * More details: https://clickhouse.tech/docs/en/operations/settings/settings-profiles/
       * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationprofiles
       *
       */
      profiles?: {
        [k: string]: any;
      };
      /**
       * allows configure <yandex><quotas>..</quotas></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
       * you can configure any aspect of resource quotas
       * More details: https://clickhouse.tech/docs/en/operations/quotas/
       * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationquotas
       *
       */
      quotas?: {
        [k: string]: any;
      };
      /**
       * allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
       * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
       * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationsettings
       *
       */
      settings?: {
        [k: string]: any;
      };
      /**
       * allows define content of any setting file inside each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
       * every key in this object is the file name
       * every value in this object is the file content
       * you can use `!!binary |` and base64 for binary files, see details here https://yaml.org/type/binary.html
       * each key could contains prefix like USERS, COMMON, HOST or config.d, users.d, cond.d, wrong prefixes will ignored, subfolders also will ignored
       * More details: https://github.com/Altinity/clickhouse-operator/blob/master/docs/chi-examples/05-settings-05-files-nested.yaml
       *
       */
      files?: {
        [k: string]: any;
      };
      /**
       * describes ClickHouse clusters layout and allows change settings on cluster-level, shard-level and replica-level
       * every cluster is a set of StatefulSet, one StatefulSet contains only one Pod with `clickhouse-server`
       * all Pods will rendered in <remote_server> part of ClickHouse configs, mounted from ConfigMap as `/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml`
       * Clusters will use for Distributed table engine, more details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
       * If `cluster` contains zookeeper settings (could be inherited from top `chi` level), when you can create *ReplicatedMergeTree tables
       *
       */
      clusters?: {
        /**
         * cluster name, used to identify set of ClickHouse servers and wide used during generate names of related Kubernetes resources
         */
        name?: string;
        /**
         * optional, allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` only in current ClickHouse cluster, during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
         * override top-level `chi.spec.configuration.zookeeper` settings
         *
         */
        zookeeper?: {
          /**
           * describe every available zookeeper cluster node for interaction
           */
          nodes?: {
            /**
             * dns name or ip address for Zookeeper node
             */
            host?: string;
            /**
             * TCP port which used to connect to Zookeeper node
             */
            port?: number;
            [k: string]: any;
          }[];
          /**
           * session timeout during connect to Zookeeper
           */
          session_timeout_ms?: number;
          /**
           * one operation timeout during Zookeeper transactions
           */
          operation_timeout_ms?: number;
          /**
           * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
           */
          root?: string;
          /**
           * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
           */
          identity?: string;
          [k: string]: any;
        };
        /**
         * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
         * override top-level `chi.spec.configuration.settings`
         * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
         *
         */
        settings?: {
          [k: string]: any;
        };
        /**
         * optional, allows define content of any setting file inside each `Pod` on current cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
         * override top-level `chi.spec.configuration.files`
         *
         */
        files?: {
          [k: string]: any;
        };
        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected cluster
         * override top-level `chi.spec.configuration.templates`
         *
         */
        templates?: {
          /**
           * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure each `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod` only for one cluster
           */
          hostTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one cluster
           */
          podTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one cluster
           */
          dataVolumeClaimTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one cluster
           */
          logVolumeClaimTemplate?: string;
          /**
           * optional, fully ignores for cluster-level
           */
          serviceTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters` only for one cluster
           */
          clusterServiceTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one cluster
           */
          shardServiceTemplate?: string;
          /**
           * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters` only for one cluster
           */
          replicaServiceTemplate?: string;
          /**
           * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
           */
          volumeClaimTemplate?: string;
          [k: string]: any;
        };
        /**
         * describe current cluster layout, how much shards in cluster, how much replica in shard
         * allows override settings on each shard and replica separatelly
         *
         */
        layout?: {
          /**
           * DEPRECATED - to be removed soon
           */
          type?: string;
          /**
           * how much shards for current ClickHouse cluster will run in Kubernetes, each shard contains shared-nothing part of data and contains set of replicas, cluster contains 1 shard by default
           */
          shardsCount?: number;
          /**
           * how much replicas in each shards for current ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance, every shard contains 1 replica by default
           */
          replicasCount?: number;
          /**
           * optional, allows override top-level `chi.spec.configuration`, cluster-level `chi.spec.configuration.clusters` settings for each shard separately, use it only if you fully understand what you do
           */
          shards?: {
            /**
             * optional, by default shard name is generated, but you can override it and setup custom name
             */
            name?: string;
            /**
             * DEPRECATED - to be removed soon
             */
            definitionType?: string;
            /**
             * optional, 1 by default, allows setup shard <weight> setting which will use during insert into tables with `Distributed` engine,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             *
             */
            weight?: number;
            /**
             * optional, `true` by default when `chi.spec.configuration.clusters[].layout.ReplicaCount` > 1 and 0 otherwise
             * allows setup <internal_replication> setting which will use during insert into tables with `Distributed` engine for insert only in one live replica and other replicas will download inserted data during replication,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             *
             */
            internalReplication?:
              | ""
              | "0"
              | "1"
              | "False"
              | "false"
              | "True"
              | "true"
              | "No"
              | "no"
              | "Yes"
              | "yes"
              | "Off"
              | "off"
              | "On"
              | "on"
              | "Disable"
              | "disable"
              | "Enable"
              | "enable"
              | "Disabled"
              | "disabled"
              | "Enabled"
              | "enabled";
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.settings` and cluster-level `chi.spec.configuration.clusters.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             *
             */
            settings?: {
              [k: string]: any;
            };
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`
             *
             */
            files?: {
              [k: string]: any;
            };
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected shard
             * override top-level `chi.spec.configuration.templates` and cluster-level `chi.spec.configuration.clusters.templates`
             *
             */
            templates?: {
              /**
               * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure each `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod` only for one shard
               */
              hostTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
               */
              podTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
               */
              dataVolumeClaimTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
               */
              logVolumeClaimTemplate?: string;
              /**
               * optional, fully ignores for shard-level
               */
              serviceTemplate?: string;
              /**
               * optional, fully ignores for shard-level
               */
              clusterServiceTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one shard
               */
              shardServiceTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one shard
               */
              replicaServiceTemplate?: string;
              /**
               * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
               */
              volumeClaimTemplate?: string;
              [k: string]: any;
            };
            /**
             * optional, how much replicas in selected shard for selected ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance,
             * shard contains 1 replica by default
             * override cluster-level `chi.spec.configuration.clusters.layout.replicasCount`
             *
             */
            replicasCount?: number;
            /**
             * optional, allows override behavior for selected replicas from cluster-level `chi.spec.configuration.clusters` and shard-level `chi.spec.configuration.clusters.layout.shards`
             *
             */
            replicas?: {
              /**
               * optional, by default replica name is generated, but you can override it and setup custom name
               */
              name?: string;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected replica, override `chi.spec.templates.hostTemplates.spec.tcpPort`
               * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
               *
               */
              tcpPort?: number;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `http` for selected replica, override `chi.spec.templates.hostTemplates.spec.httpPort`
               * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
               *
               */
              httpPort?: number;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected replica, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
               * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
               *
               */
              interserverHTTPPort?: number;
              /**
               * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
               * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and shard-level `chi.spec.configuration.clusters.layout.shards.settings`
               * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
               *
               */
              settings?: {
                [k: string]: any;
              };
              /**
               * optional, allows define content of any setting file inside `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
               * override top-level `chi.spec.configuration.files`, cluster-level `chi.spec.configuration.clusters.files` and shard-level `chi.spec.configuration.clusters.layout.shards.files`
               *
               */
              files?: {
                [k: string]: any;
              };
              /**
               * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
               * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates` and shard-level `chi.spec.configuration.clusters.layout.shards.templates`
               *
               */
              templates?: {
                /**
                 * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod` only for one replica
                 */
                hostTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one replica
                 */
                podTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
                 */
                dataVolumeClaimTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
                 */
                logVolumeClaimTemplate?: string;
                /**
                 * optional, fully ignores for replica-level
                 */
                serviceTemplate?: string;
                /**
                 * optional, fully ignores for replica-level
                 */
                clusterServiceTemplate?: string;
                /**
                 * optional, fully ignores for replica-level
                 */
                shardServiceTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one replica
                 */
                replicaServiceTemplate?: string;
                /**
                 * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
                 */
                volumeClaimTemplate?: string;
                [k: string]: any;
              };
              [k: string]: any;
            }[];
            [k: string]: any;
          }[];
          /**
           * optional, allows override top-level `chi.spec.configuration` and cluster-level `chi.spec.configuration.clusters` configuration for each replica and each shard relates to selected replica, use it only if you fully understand what you do
           */
          replicas?: {
            /**
             * optional, by default replica name is generated, but you can override it and setup custom name
             */
            name?: string;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and will ignore if shard-level `chi.spec.configuration.clusters.layout.shards` present
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             *
             */
            settings?: {
              [k: string]: any;
            };
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             *
             */
            files?: {
              [k: string]: any;
            };
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`
             *
             */
            templates?: {
              /**
               * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod` only for one replica
               */
              hostTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one replica
               */
              podTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
               */
              dataVolumeClaimTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
               */
              logVolumeClaimTemplate?: string;
              /**
               * optional, fully ignores for replica-level
               */
              serviceTemplate?: string;
              /**
               * optional, fully ignores for replica-level
               */
              clusterServiceTemplate?: string;
              /**
               * optional, fully ignores for replica-level
               */
              shardServiceTemplate?: string;
              /**
               * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one replica
               */
              replicaServiceTemplate?: string;
              /**
               * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
               */
              volumeClaimTemplate?: string;
              [k: string]: any;
            };
            /**
             * optional, count of shards related to current replica, you can override each shard behavior on low-level `chi.spec.configuration.clusters.layout.replicas.shards`
             */
            shardsCount?: number;
            /**
             * optional, list of shards related to current replica, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            shards?: {
              /**
               * optional, by default shard name is generated, but you can override it and setup custom name
               */
              name?: string;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected shard, override `chi.spec.templates.hostTemplates.spec.tcpPort`
               * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
               *
               */
              tcpPort?: number;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `http` for selected shard, override `chi.spec.templates.hostTemplates.spec.httpPort`
               * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
               *
               */
              httpPort?: number;
              /**
               * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected shard, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
               * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
               *
               */
              interserverHTTPPort?: number;
              /**
               * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
               * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and replica-level `chi.spec.configuration.clusters.layout.replicas.settings`
               * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
               *
               */
              settings?: {
                [k: string]: any;
              };
              /**
               * optional, allows define content of any setting file inside each `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
               * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
               *
               */
              files?: {
                [k: string]: any;
              };
              /**
               * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
               * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates`
               *
               */
              templates?: {
                /**
                 * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure each `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod` only for one shard
                 */
                hostTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
                 */
                podTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
                 */
                dataVolumeClaimTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters` only for one shard
                 */
                logVolumeClaimTemplate?: string;
                /**
                 * optional, fully ignores for shard-level
                 */
                serviceTemplate?: string;
                /**
                 * optional, fully ignores for shard-level
                 */
                clusterServiceTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one shard
                 */
                shardServiceTemplate?: string;
                /**
                 * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside clickhouse cluster described in `chi.spec.configuration.clusters` only for one shard
                 */
                replicaServiceTemplate?: string;
                /**
                 * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
                 */
                volumeClaimTemplate?: string;
                [k: string]: any;
              };
              [k: string]: any;
            }[];
            [k: string]: any;
          }[];
          [k: string]: any;
        };
        [k: string]: any;
      }[];
      [k: string]: any;
    };
    /**
     * allows define templates which will use for render Kubernetes resources like StatefulSet, ConfigMap, Service, PVC, by default, clickhouse-operator have own templates, but you can override it
     */
    templates?: {
      /**
       * hostTemplate will use during apply to generate `clickhose-server` config files
       */
      hostTemplates?: {
        /**
         * template name, could use to link inside top-level `chi.spec.defaults.templates.hostTemplate`, cluster-level `chi.spec.configuration.clusters.templates.hostTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.hostTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.hostTemplate`
         */
        name?: string;
        /**
         * define how will distribute numeric values of named ports in `Pod.spec.containers.ports` and clickhouse-server configs
         */
        portDistribution?: {
          /**
           * type of distribution, when `Unspecified` (default value) then all listen ports on clickhouse-server configuration in all Pods will have the same value, when `ClusterScopeIndex` then ports will increment to offset from base value depends on shard and replica index inside cluster with combination of `chi.spec.templates.podTemlates.spec.HostNetwork` it allows setup ClickHouse cluster inside Kubernetes and provide access via external network bypass Kubernetes internal network
           */
          type?: "" | "Unspecified" | "ClusterScopeIndex";
          [k: string]: any;
        }[];
        spec?: {
          /**
           * by default, hostname will generate, but this allows define custom name for each `clickhuse-server`
           */
          name?: string;
          /**
           * optional, setup `tcp_port` inside `clickhouse-server` settings for each Pod where current template will apply
           * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=tcp]`
           * More info: https://clickhouse.tech/docs/en/interfaces/tcp/
           *
           */
          tcpPort?: number;
          /**
           * optional, setup `http_port` inside `clickhouse-server` settings for each Pod where current template will apply
           * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=http]`
           * More info: https://clickhouse.tech/docs/en/interfaces/http/
           *
           */
          httpPort?: number;
          /**
           * optional, setup `interserver_http_port` inside `clickhouse-server` settings for each Pod where current template will apply
           * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=interserver]`
           * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#interserver-http-port
           *
           */
          interserverHTTPPort?: number;
          /**
           * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
           * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
           *
           */
          settings?: {
            [k: string]: any;
          };
          /**
           * optional, allows define content of any setting file inside each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
           *
           */
          files?: {
            [k: string]: any;
          };
          /**
           * be carefull, this part of CRD allows override template inside template, don't use it if you don't understand what you do
           */
          templates?: {
            hostTemplate?: string;
            podTemplate?: string;
            dataVolumeClaimTemplate?: string;
            logVolumeClaimTemplate?: string;
            serviceTemplate?: string;
            clusterServiceTemplate?: string;
            shardServiceTemplate?: string;
            replicaServiceTemplate?: string;
            [k: string]: any;
          };
          [k: string]: any;
        };
        [k: string]: any;
      }[];
      /**
       * podTemplate will use during render `Pod` inside `StatefulSet.spec` and allows define rendered `Pod.spec`, pod scheduling distribution and pod zone
       * More information: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatespodtemplates
       *
       */
      podTemplates?: {
        /**
         * template name, could use to link inside top-level `chi.spec.defaults.templates.podTemplate`, cluster-level `chi.spec.configuration.clusters.templates.podTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.podTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.podTemplate`
         */
        name?: string;
        /**
         * allows define format for generated `Pod` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
         */
        generateName?: string;
        /**
         * allows define custom zone name and will separate ClickHouse `Pods` between nodes, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
         */
        zone?: {
          /**
           * optional, if defined, allows select kubernetes nodes by label with `name` equal `key`
           */
          key?: string;
          /**
           * optional, if defined, allows select kubernetes nodes by label with `value` in `values`
           */
          values?: string[];
          [k: string]: any;
        };
        /**
         * DEPRECATED, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
         */
        distribution?: "" | "Unspecified" | "OnePerHost";
        /**
         * define ClickHouse Pod distibution policy between Kubernetes Nodes inside Shard, Replica, Namespace, CHI, another ClickHouse cluster
         */
        podDistribution?: {
          /**
           * you can define multiple affinity policy types
           */
          type?:
            | ""
            | "Unspecified"
            | "ClickHouseAntiAffinity"
            | "ShardAntiAffinity"
            | "ReplicaAntiAffinity"
            | "AnotherNamespaceAntiAffinity"
            | "AnotherClickHouseInstallationAntiAffinity"
            | "AnotherClusterAntiAffinity"
            | "MaxNumberPerNode"
            | "NamespaceAffinity"
            | "ClickHouseInstallationAffinity"
            | "ClusterAffinity"
            | "ShardAffinity"
            | "ReplicaAffinity"
            | "PreviousTailAffinity"
            | "CircularReplication";
          /**
           * scope for apply each podDistribution
           */
          scope?: "" | "Unspecified" | "Shard" | "Replica" | "Cluster" | "ClickHouseInstallation" | "Namespace";
          /**
           * define, how much ClickHouse Pods could be inside selected scope with selected distribution type
           */
          number?: number;
          /**
           * use for inter-pod affinity look to `pod.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution.podAffinityTerm.topologyKey`, More info: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
           */
          topologyKey?: string;
          [k: string]: any;
        }[];
        /**
         * allows define whole Pod.spec inside StaefulSet.spec, look to https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates for details
         */
        spec?: {
          [k: string]: any;
        };
        /**
         * allows pass standard object's metadata from template to Pod
         * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
         *
         */
        metadata?: {
          [k: string]: any;
        };
        [k: string]: any;
      }[];
      /**
       * allows define template for rendering `PVC` kubernetes resource, which would use inside `Pod` for mount clickhouse `data`, clickhouse `logs` or something else
       */
      volumeClaimTemplates?: {
        /**
         * template name, could use to link inside
         * top-level `chi.spec.defaults.templates.dataVolumeClaimTemplate` or `chi.spec.defaults.templates.logVolumeClaimTemplate`,
         * cluster-level `chi.spec.configuration.clusters.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.templates.logVolumeClaimTemplate`,
         * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.shards.temlates.logVolumeClaimTemplate`
         * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.replicas.templates.logVolumeClaimTemplate`
         *
         */
        name?: string;
        /**
         * define behavior of `PVC` deletion policy during delete `Pod`, `Delete` by default, when `Retain` then `PVC` still alive even `Pod` will deleted
         */
        reclaimPolicy?: "" | "Retain" | "Delete";
        /**
         * allows pass standard object's metadata from template to PVC
         * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
         *
         */
        metadata?: {
          [k: string]: any;
        };
        /**
         * allows define all aspects of `PVC` resource
         * More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
         *
         */
        spec?: {
          [k: string]: any;
        };
        [k: string]: any;
      }[];
      /**
       * allows define template for rendering `Service` which would get endpoint from Pods which scoped chi-wide, cluster-wide, shard-wide, replica-wide level
       *
       */
      serviceTemplates?: {
        /**
         * template name, could use to link inside
         * chi-level `chi.spec.defaults.templates.serviceTemplate`
         * cluster-level `chi.spec.configuration.clusters.templates.clusterServiceTemplate`
         * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.shardServiceTemplate`
         * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.replicaServiceTemplate` or `chi.spec.configuration.clusters.layout.shards.replicas.replicaServiceTemplate`
         *
         */
        name?: string;
        /**
         * allows define format for generated `Service` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
         */
        generateName?: string;
        /**
         * allows pass standard object's metadata from template to Service
         * Could be use for define specificly for Cloud Provider metadata which impact to behavior of service
         * More info: https://kubernetes.io/docs/concepts/services-networking/service/
         *
         */
        metadata?: {
          [k: string]: any;
        };
        /**
         * describe behavior of generated Service
         * More info: https://kubernetes.io/docs/concepts/services-networking/service/
         *
         */
        spec?: {
          [k: string]: any;
        };
        [k: string]: any;
      }[];
      [k: string]: any;
    };
    /**
     * list of `ClickHouseInstallationTemplate` (chit) resource names which will merge with current `Chi` manifest during render Kubernetes resources to create related ClickHouse clusters
     */
    useTemplates?: {
      /**
       * name of `ClickHouseInstallationTemplate` (chit) resource
       */
      name?: string;
      /**
       * Kubernetes namespace where need search `chit` resource, depending on `watchNamespaces` settings in `clichouse-operator`
       */
      namespace?: string;
      /**
       * optional, current strategy is only merge, and current `chi` settings have more priority than merged template `chit`
       */
      useType?: "" | "merge";
      [k: string]: any;
    }[];
    [k: string]: any;
  };
  [k: string]: any;
}

export interface V1ClickhouseinstallationtemplateList {
  /**
    * APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
    */
   apiVersion?: string;
   /**
   * Items is the list of ControllerRevisions
   */
   items: Array<V1Clickhouseinstallationtemplate>;
   /**
   * Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
   */
   kind?: string;
   metadata?: V1ListMeta;
}

let defaultBasePath = 'http://localhost';

export enum V1ClickhouseinstallationtemplateApiApiKeys {
    BearerToken,
}

export class V1ClickhouseinstallationtemplateApi {
  protected _basePath = defaultBasePath;
  protected _defaultHeaders : any = {};
  protected _useQuerystring : boolean = false;

  protected authentications = {
      'default': <Authentication>new VoidAuth(),
      'BearerToken': new ApiKeyAuth('header', 'authorization'),
  }

  protected interceptors: Interceptor[] = [];

  constructor(basePath?: string);
  constructor(basePathOrUsername: string, password?: string, basePath?: string) {
      if (password) {
          if (basePath) {
              this.basePath = basePath;
          }
      } else {
          if (basePathOrUsername) {
              this.basePath = basePathOrUsername
          }
      }
  }

  set useQuerystring(value: boolean) {
      this._useQuerystring = value;
  }

  set basePath(basePath: string) {
      this._basePath = basePath;
  }

  set defaultHeaders(defaultHeaders: any) {
      this._defaultHeaders = defaultHeaders;
  }

  get defaultHeaders() {
      return this._defaultHeaders;
  }

  get basePath() {
      return this._basePath;
  }

  public setDefaultAuthentication(auth: Authentication) {
      this.authentications.default = auth;
  }

  public setApiKey(key: V1ClickhouseinstallationtemplateApiApiKeys, value: string) {
    (this.authentications as any)[V1ClickhouseinstallationtemplateApiApiKeys[key]].apiKey = value;
  }

  public addInterceptor(interceptor: Interceptor) {
      this.interceptors.push(interceptor);
  }

  /**
   * create a V1Clickhouseinstallationtemplate
   * @param namespace object name and auth scope, such as for teams and projects
   * @param body
   * @param includeUninitialized If true, partially initialized resources are included in the response.
   * @param pretty If &#39;true&#39;, then the output is pretty printed.
   * @param dryRun When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
   */
  public async createNamespacedV1Clickhouseinstallationtemplate (namespace: string, body: V1Clickhouseinstallationtemplate, includeUninitialized?: boolean, pretty?: string, dryRun?: string, options: {headers: {[name: string]: string}} = {headers: {}}) : Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }> {
    const localVarPath = this.basePath + '/apis/clickhouse.altinity.com/v1/namespaces/{namespace}/clickhouseinstallationtemplates'
        .replace('{' + 'namespace' + '}', encodeURIComponent(String(namespace)));
    let localVarQueryParameters: any = {};
    let localVarHeaderParams: any = (<any>Object).assign({}, this.defaultHeaders);
    let localVarFormParams: any = {};

    // verify required parameter 'namespace' is not null or undefined
    if (namespace === null || namespace === undefined) {
        throw new Error('Required parameter namespace was null or undefined when calling createNamespacedV1Clickhouseinstallationtemplate.');
    }

    // verify required parameter 'body' is not null or undefined
    if (body === null || body === undefined) {
        throw new Error('Required parameter body was null or undefined when calling createNamespacedV1Clickhouseinstallationtemplate.');
    }

    if (includeUninitialized !== undefined) {
        localVarQueryParameters['includeUninitialized'] = includeUninitialized;
    }

    if (pretty !== undefined) {
        localVarQueryParameters['pretty'] = pretty;
    }

    if (dryRun !== undefined) {
        localVarQueryParameters['dryRun'] = dryRun;
    }

    (<any>Object).assign(localVarHeaderParams, options.headers);

    let localVarUseFormData = false;

    let localVarRequestOptions: localVarRequest.Options = {
        method: 'POST',
        qs: localVarQueryParameters,
        headers: localVarHeaderParams,
        uri: localVarPath,
        useQuerystring: this._useQuerystring,
        json: true,
        body: body
    };

    let authenticationPromise = Promise.resolve();
    authenticationPromise = authenticationPromise.then(() => this.authentications.BearerToken.applyToRequest(localVarRequestOptions));

    authenticationPromise = authenticationPromise.then(() => this.authentications.default.applyToRequest(localVarRequestOptions));
    return authenticationPromise.then(() => {
        if (Object.keys(localVarFormParams).length) {
            if (localVarUseFormData) {
                (<any>localVarRequestOptions).formData = localVarFormParams;
            } else {
                localVarRequestOptions.form = localVarFormParams;
            }
        }
        return new Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }>((resolve, reject) => {
            localVarRequest(localVarRequestOptions, (error, response, body) => {
                if (error) {
                    reject(error);
                } else {
                    if (response.statusCode && response.statusCode >= 200 && response.statusCode <= 299) {
                        resolve({ response: response, body: body });
                    } else {
                        reject({ response: response, body: body });
                    }
                }
            });
        });
    });
  }

  /**
   * read the specified V1Clickhouseinstallationtemplate
   * @param name name of the V1Clickhouseinstallationtemplate
   * @param namespace object name and auth scope, such as for teams and projects
   * @param pretty If &#39;true&#39;, then the output is pretty printed.
   * @param exact Should the export be exact.  Exact export maintains cluster-specific fields like &#39;Namespace&#39;.
   * @param _export Should this value be exported.  Export strips fields that a user can not specify.
   */
  public async readNamespacedV1Clickhouseinstallationtemplate (name: string, namespace: string, pretty?: string, exact?: boolean, _export?: boolean, options: {headers: {[name: string]: string}} = {headers: {}}) : Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }> {
    const localVarPath = this.basePath + '/apis/clickhouse.altinity.com/v1/namespaces/{namespace}/clickhouseinstallationtemplates/{name}'
      .replace('{' + 'name' + '}', encodeURIComponent(String(name)))
      .replace('{' + 'namespace' + '}', encodeURIComponent(String(namespace)));
    let localVarQueryParameters: any = {};
    let localVarHeaderParams: any = (<any>Object).assign({}, this.defaultHeaders);
    let localVarFormParams: any = {};

    // verify required parameter 'name' is not null or undefined
    if (name === null || name === undefined) {
        throw new Error('Required parameter name was null or undefined when calling readNamespacedV1Clickhouseinstallationtemplate.');
    }

    // verify required parameter 'namespace' is not null or undefined
    if (namespace === null || namespace === undefined) {
        throw new Error('Required parameter namespace was null or undefined when calling readNamespacedV1Clickhouseinstallationtemplate.');
    }

    if (pretty !== undefined) {
      localVarQueryParameters['pretty'] = pretty;
    }

    if (exact !== undefined) {
        localVarQueryParameters['exact'] = exact;
    }

    if (_export !== undefined) {
        localVarQueryParameters['export'] = _export;
    }

    (<any>Object).assign(localVarHeaderParams, options.headers);

    let localVarUseFormData = false;

    let localVarRequestOptions: localVarRequest.Options = {
        method: 'GET',
        qs: localVarQueryParameters,
        headers: localVarHeaderParams,
        uri: localVarPath,
        useQuerystring: this._useQuerystring,
        json: true,
    };

    let authenticationPromise = Promise.resolve();
    authenticationPromise = authenticationPromise.then(() => this.authentications.BearerToken.applyToRequest(localVarRequestOptions));

    authenticationPromise = authenticationPromise.then(() => this.authentications.default.applyToRequest(localVarRequestOptions));
    return authenticationPromise.then(() => {
        if (Object.keys(localVarFormParams).length) {
            if (localVarUseFormData) {
                (<any>localVarRequestOptions).formData = localVarFormParams;
            } else {
                localVarRequestOptions.form = localVarFormParams;
            }
        }
        return new Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }>((resolve, reject) => {
            localVarRequest(localVarRequestOptions, (error, response, body) => {
                if (error) {
                    reject(error);
                } else {
                    if (response.statusCode && response.statusCode >= 200 && response.statusCode <= 299) {
                        resolve({ response: response, body: body });
                    } else {
                        reject({ response: response, body: body });
                    }
                }
            });
        });
    });
  }

  /**
   * partially update the specified V1Clickhouseinstallationtemplate
   * @param name name of the V1Clickhouseinstallationtemplate
   * @param namespace object name and auth scope, such as for teams and projects
   * @param body
   * @param pretty If &#39;true&#39;, then the output is pretty printed.
   * @param dryRun When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
   */
  public async patchNamespacedV1Clickhouseinstallationtemplate (name: string, namespace: string, body: object, pretty?: string, dryRun?: string, options: {headers: {[name: string]: string}} = {headers: {}}) : Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }> {
    const localVarPath = this.basePath + '/apis/clickhouse.altinity.com/v1/namespaces/{namespace}/clickhouseinstallationtemplates/{name}'
        .replace('{' + 'name' + '}', encodeURIComponent(String(name)))
        .replace('{' + 'namespace' + '}', encodeURIComponent(String(namespace)));
    let localVarQueryParameters: any = {};
    let localVarHeaderParams: any = (<any>Object).assign({}, this.defaultHeaders);
    let localVarFormParams: any = {};

    // verify required parameter 'name' is not null or undefined
    if (name === null || name === undefined) {
        throw new Error('Required parameter name was null or undefined when calling patchNamespacedV1Clickhouseinstallationtemplate.');
    }

    // verify required parameter 'namespace' is not null or undefined
    if (namespace === null || namespace === undefined) {
        throw new Error('Required parameter namespace was null or undefined when calling patchNamespacedV1Clickhouseinstallationtemplate.');
    }

    // verify required parameter 'body' is not null or undefined
    if (body === null || body === undefined) {
        throw new Error('Required parameter body was null or undefined when calling patchNamespacedV1Clickhouseinstallationtemplate.');
    }

    if (pretty !== undefined) {
        localVarQueryParameters['pretty'] = pretty;
    }

    if (dryRun !== undefined) {
        localVarQueryParameters['dryRun'] = dryRun;
    }

    (<any>Object).assign(localVarHeaderParams, options.headers);

    let localVarUseFormData = false;

    let localVarRequestOptions: localVarRequest.Options = {
        method: 'PATCH',
        qs: localVarQueryParameters,
        headers: localVarHeaderParams,
        uri: localVarPath,
        useQuerystring: this._useQuerystring,
        json: true,
        body: body
    };

    let authenticationPromise = Promise.resolve();
    authenticationPromise = authenticationPromise.then(() => this.authentications.BearerToken.applyToRequest(localVarRequestOptions));

    authenticationPromise = authenticationPromise.then(() => this.authentications.default.applyToRequest(localVarRequestOptions));
    return authenticationPromise.then(() => {
        if (Object.keys(localVarFormParams).length) {
            if (localVarUseFormData) {
                (<any>localVarRequestOptions).formData = localVarFormParams;
            } else {
                localVarRequestOptions.form = localVarFormParams;
            }
        }
        return new Promise<{ response: IncomingMessage; body: V1Clickhouseinstallationtemplate;  }>((resolve, reject) => {
            localVarRequest(localVarRequestOptions, (error, response, body) => {
                if (error) {
                    reject(error);
                } else {
                    if (response.statusCode && response.statusCode >= 200 && response.statusCode <= 299) {
                        resolve({ response: response, body: body });
                    } else {
                        reject({ response: response, body: body });
                    }
                }
            });
        });
    });
  }

  /**
   * delete a V1Clickhouseinstallationtemplate
   * @param name name of the V1Clickhouseinstallationtemplate
   * @param namespace object name and auth scope, such as for teams and projects
   * @param pretty If &#39;true&#39;, then the output is pretty printed.
   * @param dryRun When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
   * @param gracePeriodSeconds The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
   * @param orphanDependents Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object&#39;s finalizers list. Either this field or PropagationPolicy may be set, but not both.
   * @param propagationPolicy Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: &#39;Orphan&#39; - orphan the dependents; &#39;Background&#39; - allow the garbage collector to delete the dependents in the background; &#39;Foreground&#39; - a cascading policy that deletes all dependents in the foreground.
   * @param body
   */
  public async deleteNamespacedV1Clickhouseinstallationtemplate (name: string, namespace: string, pretty?: string, dryRun?: string, gracePeriodSeconds?: number, orphanDependents?: boolean, propagationPolicy?: string, body?: any, options: {headers: {[name: string]: string}} = {headers: {}}) : Promise<{ response: IncomingMessage; body: V1Status;  }> {
    const localVarPath = this.basePath + '/apis/clickhouse.altinity.com/v1/namespaces/{namespace}/clickhouseinstallationtemplates/{name}'
        .replace('{' + 'name' + '}', encodeURIComponent(String(name)))
        .replace('{' + 'namespace' + '}', encodeURIComponent(String(namespace)));
    let localVarQueryParameters: any = {};
    let localVarHeaderParams: any = (<any>Object).assign({}, this.defaultHeaders);
    let localVarFormParams: any = {};

    // verify required parameter 'name' is not null or undefined
    if (name === null || name === undefined) {
        throw new Error('Required parameter name was null or undefined when calling deleteNamespacedV1Clickhouseinstallationtemplate.');
    }

    // verify required parameter 'namespace' is not null or undefined
    if (namespace === null || namespace === undefined) {
        throw new Error('Required parameter namespace was null or undefined when calling deleteNamespacedV1Clickhouseinstallationtemplate.');
    }

    if (pretty !== undefined) {
        localVarQueryParameters['pretty'] = pretty;
    }

    if (dryRun !== undefined) {
        localVarQueryParameters['dryRun'] = dryRun;
    }

    if (gracePeriodSeconds !== undefined) {
        localVarQueryParameters['gracePeriodSeconds'] = gracePeriodSeconds;
    }

    if (orphanDependents !== undefined) {
        localVarQueryParameters['orphanDependents'] = orphanDependents;
    }

    if (propagationPolicy !== undefined) {
        localVarQueryParameters['propagationPolicy'] = propagationPolicy;
    }

    (<any>Object).assign(localVarHeaderParams, options.headers);

    let localVarUseFormData = false;

    let localVarRequestOptions: localVarRequest.Options = {
        method: 'DELETE',
        qs: localVarQueryParameters,
        headers: localVarHeaderParams,
        uri: localVarPath,
        useQuerystring: this._useQuerystring,
        json: true,
        body: {}
    };

    let authenticationPromise = Promise.resolve();
    authenticationPromise = authenticationPromise.then(() => this.authentications.BearerToken.applyToRequest(localVarRequestOptions));

    authenticationPromise = authenticationPromise.then(() => this.authentications.default.applyToRequest(localVarRequestOptions));
    return authenticationPromise.then(() => {
        if (Object.keys(localVarFormParams).length) {
            if (localVarUseFormData) {
                (<any>localVarRequestOptions).formData = localVarFormParams;
            } else {
                localVarRequestOptions.form = localVarFormParams;
            }
        }
        return new Promise<{ response: IncomingMessage; body: V1Status;  }>((resolve, reject) => {
            localVarRequest(localVarRequestOptions, (error, response, body) => {
                if (error) {
                    reject(error);
                } else {
                    body = ObjectSerializer.deserialize(body, "V1Status");
                    if (response.statusCode && response.statusCode >= 200 && response.statusCode <= 299) {
                        resolve({ response: response, body: body });
                    } else {
                        reject({ response: response, body: body });
                    }
                }
            });
        });
    });
  }

  /**
   * list or watch objects of kind V1Clickhouseinstallationtemplate
   * @param allowWatchBookmarks allowWatchBookmarks requests watch events with type &quot;BOOKMARK&quot;. Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server&#39;s discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored. If the feature gate WatchBookmarks is not enabled in apiserver, this field is ignored.  This field is alpha and can be changed or removed without notice.
   * @param _continue The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the &quot;next key&quot;.  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
   * @param fieldSelector A selector to restrict the list of returned objects by their fields. Defaults to everything.
   * @param labelSelector A selector to restrict the list of returned objects by their labels. Defaults to everything.
   * @param limit limit is a maximum number of responses to return for a list call. If more items exist, the server will set the &#x60;continue&#x60; field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
   * @param pretty If &#39;true&#39;, then the output is pretty printed.
   * @param resourceVersion When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it&#39;s 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
   * @param timeoutSeconds Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
   * @param watch Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
   */
  public async listV1ClickhouseinstallationtemplateForAllNamespaces (allowWatchBookmarks?: boolean, _continue?: string, fieldSelector?: string, labelSelector?: string, limit?: number, pretty?: string, resourceVersion?: string, timeoutSeconds?: number, watch?: boolean, options: {headers: {[name: string]: string}} = {headers: {}}) : Promise<{ response: IncomingMessage; body: V1ClickhouseinstallationtemplateList;  }> {
      const localVarPath = this.basePath + '/apis/clickhouse.altinity.com/v1/clickhouseinstallationtemplates';
      let localVarQueryParameters: any = {};
      let localVarHeaderParams: any = (<any>Object).assign({}, this.defaultHeaders);
      let localVarFormParams: any = {};

      if (allowWatchBookmarks !== undefined) {
          localVarQueryParameters['allowWatchBookmarks'] = allowWatchBookmarks;
      }

      if (_continue !== undefined) {
          localVarQueryParameters['continue'] = _continue;
      }

      if (fieldSelector !== undefined) {
          localVarQueryParameters['fieldSelector'] = fieldSelector;
      }

      if (labelSelector !== undefined) {
          localVarQueryParameters['labelSelector'] = labelSelector;
      }

      if (limit !== undefined) {
          localVarQueryParameters['limit'] = limit;
      }

      if (pretty !== undefined) {
          localVarQueryParameters['pretty'] = pretty;
      }

      if (resourceVersion !== undefined) {
          localVarQueryParameters['resourceVersion'] = resourceVersion;
      }

      if (timeoutSeconds !== undefined) {
          localVarQueryParameters['timeoutSeconds'] = timeoutSeconds;
      }

      if (watch !== undefined) {
          localVarQueryParameters['watch'] = watch;
      }

      (<any>Object).assign(localVarHeaderParams, options.headers);

      let localVarUseFormData = false;

      let localVarRequestOptions: localVarRequest.Options = {
          method: 'GET',
          qs: localVarQueryParameters,
          headers: localVarHeaderParams,
          uri: localVarPath,
          useQuerystring: this._useQuerystring,
          json: true,
      };

      let authenticationPromise = Promise.resolve();
      authenticationPromise = authenticationPromise.then(() => this.authentications.BearerToken.applyToRequest(localVarRequestOptions));

      authenticationPromise = authenticationPromise.then(() => this.authentications.default.applyToRequest(localVarRequestOptions));
      return authenticationPromise.then(() => {
          if (Object.keys(localVarFormParams).length) {
              if (localVarUseFormData) {
                  (<any>localVarRequestOptions).formData = localVarFormParams;
              } else {
                  localVarRequestOptions.form = localVarFormParams;
              }
          }
          return new Promise<{ response: IncomingMessage; body: V1ClickhouseinstallationtemplateList;  }>((resolve, reject) => {
              localVarRequest(localVarRequestOptions, (error, response, body) => {
                  if (error) {
                      reject(error);
                  } else {
                      if (response.statusCode && response.statusCode >= 200 && response.statusCode <= 299) {
                          resolve({ response: response, body: body });
                      } else {
                          reject({ response: response, body: body });
                      }
                  }
              });
          });
      });
  }};

export type V1ClickhouseinstallationtemplateResourceType = V1ClickhouseinstallationtemplateResource;
export type V1ClickhouseinstallationtemplateResources = V1ClickhouseinstallationtemplateResourceType[];

export const isV1ClickhouseinstallationtemplateResource = <(r: K8sResource) => r is V1ClickhouseinstallationtemplateResourceType>(
  (resource => resource instanceof V1ClickhouseinstallationtemplateResource)
);

export const V1ClickhouseinstallationtemplateActions = {
  fetch: createAsyncAction(
    "FETCH_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATES_REQUEST",
    "FETCH_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATES_SUCCESS",
    "FETCH_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATES_FAILURE"
  )<{}, { resources: V1ClickhouseinstallationtemplateResources }, { error: Error }>(),
  onUpdated: createAction("ON_UPDATED_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATE")<V1ClickhouseinstallationtemplateResourceType>(),
  onAdded: createAction("ON_ADDED_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATE")<V1ClickhouseinstallationtemplateResourceType>(),
  onDestroyed: createAction("ON_DESTROYED_K8S_V1CLICKHOUSEINSTALLATIONTEMPLATE")<V1ClickhouseinstallationtemplateResourceType>()
};
export type V1ClickhouseinstallationtemplateResourceActions = ActionType<typeof V1ClickhouseinstallationtemplateActions>;
export interface V1ClickhouseinstallationtemplateResourceState extends ResourceCache<V1ClickhouseinstallationtemplateResourceType> {}

const initialState: V1ClickhouseinstallationtemplateResourceState = {
  loaded: false,
  error: null,
  resources: []
};

export const V1ClickhouseinstallationtemplateReducer = createReducer<
  V1ClickhouseinstallationtemplateResourceState,
  V1ClickhouseinstallationtemplateResourceActions
>(initialState)
  .handleAction(
    V1ClickhouseinstallationtemplateActions.fetch.request,
    (state, _): V1ClickhouseinstallationtemplateResourceState => ({
      ...state,
      loaded: false
    })
  )
  .handleAction(
    V1ClickhouseinstallationtemplateActions.fetch.success,
    (state, action): V1ClickhouseinstallationtemplateResourceState => ({
      ...state,
      ...action.payload,
      error: null,
      loaded: true
    })
  )
  .handleAction(
    V1ClickhouseinstallationtemplateActions.fetch.failure,
    (state, action): V1ClickhouseinstallationtemplateResourceState => ({
      ...state,
      ...action.payload,
      loaded: false
    })
  )
  .handleAction(
    [V1ClickhouseinstallationtemplateActions.onUpdated, V1ClickhouseinstallationtemplateActions.onAdded],
    (state, action): V1ClickhouseinstallationtemplateResourceState => ({
      ...state,
      resources: [
        ...state.resources.filter(
          s => !isSameObject(s, action.payload)
        ),
        action.payload
      ]
    })
  )
  .handleAction(
    V1ClickhouseinstallationtemplateActions.onDestroyed,
    (state, action): V1ClickhouseinstallationtemplateResourceState => ({
      ...state,
      resources: state.resources.filter(
        s => !isSameObject(s, action.payload)
      )
    })
  );

export class V1ClickhouseinstallationtemplateResource extends K8sResource {
  protected api: V1ClickhouseinstallationtemplateApi;
  protected resource: V1Clickhouseinstallationtemplate;

  constructor(resource: V1Clickhouseinstallationtemplate, kubeConfig: KubeConfig) {
    super(resource, kubeConfig);

    this.resource = resource;
    this.api = kubeConfig.makeApiClient(V1ClickhouseinstallationtemplateApi);
  }
  get spec(): V1Clickhouseinstallationtemplate {
    return this.resource;
  }
  static startInformer(
    kubeConfig: KubeConfig,
    channel: (input: unknown) => void
  ): () => void {
    const client = kubeConfig.makeApiClient(V1ClickhouseinstallationtemplateApi);
    let cancelled = false;
    let request: Request;
    //@ts-ignore: error TS7023: 'watch' implicitly has return type 'any'
    const watch = async () => {
      if (cancelled) {
        return;
      }
      try {
        const res = await client.listV1ClickhouseinstallationtemplateForAllNamespaces();
        channel(
          V1ClickhouseinstallationtemplateActions.fetch.success({
            resources: res.body.items.map(r => new V1ClickhouseinstallationtemplateResource(r, kubeConfig))
          })
        );
      } catch (error: any) {
        channel(V1ClickhouseinstallationtemplateActions.fetch.failure({ error }));
        log.warning("starting informer failed (will retry):  %s", error);
        return setTimeout(watch, 3000);
      }
      const informer = new Watch(kubeConfig);
      const watchHandler = (phase: string, obj: V1Clickhouseinstallationtemplate) => {
        switch (phase) {
          case "ADDED":
            channel(V1ClickhouseinstallationtemplateActions.onAdded(new V1ClickhouseinstallationtemplateResource(obj, kubeConfig)));
            break;
          case "MODIFIED":
            channel(V1ClickhouseinstallationtemplateActions.onUpdated(new V1ClickhouseinstallationtemplateResource(obj, kubeConfig)));
            break;
          case "DELETED":
            channel(
              V1ClickhouseinstallationtemplateActions.onDestroyed(new V1ClickhouseinstallationtemplateResource(obj, kubeConfig))
            );
            break;
        }
      };
      request = await informer.watch(
        "/apis/clickhouse.altinity.com/v1/clickhouseinstallationtemplates",
        { resourceVersion: undefined },
        watchHandler,
        watch
      );
      return request;
    };
    watch();
    // Return a function to disable the informer and close the request
    return () => {
      cancelled = true;
      request && request.abort();
    };
  }
  create(): Promise<{
    response: IncomingMessage;
    body: V1Clickhouseinstallationtemplate;
  }> {
    return this.api.createNamespacedV1Clickhouseinstallationtemplate(this.namespace, this.resource)

  }
  read(): Promise<{
    response: IncomingMessage;
    body: V1Clickhouseinstallationtemplate;
  }> {
    return this.api.readNamespacedV1Clickhouseinstallationtemplate(this.name, this.namespace)

  }
  update(): Promise<{
    response: IncomingMessage;
    body: V1Clickhouseinstallationtemplate;
  }> {
    return this.api.patchNamespacedV1Clickhouseinstallationtemplate(
        this.name,
        this.namespace,
        this.resource,
        undefined,
        undefined,
        { headers: { "Content-Type": "application/merge-patch+json" } }
      )

  }
  delete(): Promise<{
    response: IncomingMessage;
    body: V1Status;
  }> {
    return this.api.deleteNamespacedV1Clickhouseinstallationtemplate(this.name, this.namespace)
  }
}
